# Loan Default Prediction Project
# ==================================

# Import necessary libraries
import pandas as pd

# Sample submission
# test_df shows eCommerce transactions and associated loans from 31 March 2019 to 17 July 2019.
# Variables associated with whether a customer defaulted on the loan and additional loan details are removed.
test_df = pd.read_csv('/Users/chagwon-u/Desktop/Project/Loan Default Prediction Challenge/test.csv')

# train_df contains eCommerce transactions and associated loans from 21 September 2018 to 31 March 2019.
# This dataset includes whether or not a customer defaulted on their loan.
train_df = pd.read_csv('/Users/chagwon-u/Desktop/Project/Loan Default Prediction Challenge/train.csv')

# masked_final contains e-commerce transactions not linked to loans, but associated with customers who have loan-linked transactions.
masked_final = pd.read_csv('/Users/chagwon-u/Desktop/Project/Loan Default Prediction Challenge/unlinked_masked_final.csv')

# Compare columns between train_df and masked_final
columns_train = set(train_df.columns)
columns_masked = set(masked_final.columns)

# Columns only in the training data
columns_in_original_data_only = columns_train - columns_masked
print("Columns only in train_df:", columns_in_original_data_only)

# Columns only in the masked final data
columns_in_masked_final_only = columns_masked - columns_train
print("Columns only in masked_final:", columns_in_masked_final_only)

# Check train_df info
train_df.info()

# Check for missing values in train_df
print(train_df.isnull().sum())

# Generate a pandas profiling report for the train_df dataset
import pandas_profiling
from pandas_profiling import ProfileReport

profile = pandas_profiling.ProfileReport(train_df, title="Data Profile Report", minimal=True)
profile.to_file(output_file="profile.html")

# Remove rows with missing values
train_df_cleaned = train_df.dropna(inplace=False)
train_df_cleaned.info()

# One-hot encode categorical features (ProductCategory)
import category_encoders as ce

oce = ce.OneHotEncoder(cols=['ProductCategory'])
df_encoded = oce.fit_transform(train_df_cleaned)

# Feature Selection
# ===================
# Select the target variable
target = 'IsDefaulted'

# Correlation analysis with target variable
correlations = train_df_cleaned.corrwith(train_df_cleaned[target])
correlations_sorted = correlations.sort_values(ascending=False)
print(correlations_sorted)

# Model Training and Testing
# ===========================
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# Define features and target for training
X_train = train_df_cleaned[['Value', 'Amount']]
y_train = train_df_cleaned['IsDefaulted']

# Split the dataset into training and validation sets (80/20 split)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Normalize the feature data
scaler = StandardScaler()
scaler.fit(X_train)

# Transform the training and validation data
X_train_scaled = scaler.transform(X_train)
X_val_scaled = scaler.transform(X_val)

# Logistic Regression Model
# Train the logistic regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Apply the scaler to the test data
X_test_scaled = scaler.transform(test_df[['Value', 'Amount']])

# Predict using the trained model
new_predictions = model.predict(X_test_scaled)

# Display predictions
print(new_predictions)
